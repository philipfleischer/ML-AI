{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f792e338",
   "metadata": {},
   "source": [
    "# IN1160 – Oblig 1a: Vektorrom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730524db",
   "metadata": {},
   "source": [
    "**Våren 2026**\n",
    "\n",
    "Det er en god idé å lese gjennom hele oppgavesettet før dere setter i gang.\n",
    "Dersom dere har spørsmål så kan dere:\n",
    "\n",
    "- spørre i gruppetimene,\n",
    "- spørre på Discourse\n",
    "- eller sende epost til in1160-support@ifi.uio.no dersom alternativene over av en eller annen grunn ikke passer for spørsmålet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902bec1",
   "metadata": {},
   "source": [
    "## Innlevering\n",
    "\n",
    "Oppgaven leveres innen 11.02. klokken 23:59 i [Devilry](https://devilry.ifi.uio.no/). \n",
    "\n",
    "Innleveringen skal bestå av én Jupyter notebook med både kode og tilhørende forklaringer. **Før innlevering skal du kjøre gjennom hele notebooken, før du lagrer siste gang. Den bør kjøre uten å feile og vise den grafikken og de utskriftene som skal være med.**\n",
    "Skulle dere av tekniske grunner ikke kunne besvare alle spørsmålene i en notebook kan dere i denne obligen også levere vanlige Python-filer, sammen med en PDF som inneholder grafikker og resultater fra kjøring, samt deres tekst-besvarelser. Ta kontakt med emnestaben, så vi får ordnet eventuelle problemer før oblig 1b.\n",
    "\n",
    "Vi understreker at innlevering av kode alene ikke er nok for å bestå oppgaven – vi forventer at notebooken også skal inneholde kommentarer (på norsk eller engelsk) på hva dere har gjort og begrunnelser for valgene dere har tatt underveis.\n",
    "La enhver oblig bli en trening i å formidle forskning. Bruk helst hele setninger, og matematiske formler om nødvendig. Resultater skal presenteres i tabeller på en oversiktlig måte.\n",
    "Det å forklare med egne ord, bruke begreper vi har gått gjennom på forelesningene og å forklare og reflektere over løsningene deres er en viktig del av læringsprosessen – ta det på alvor!\n",
    "\n",
    "Når det gjelder bruk av generative prateroboter (ChatGPT og lignende): Dere kan bruke dem som en \"sparringspartner\", for eksempel for å forklare noe dere ikke helt har forstått. Dere har imidlertid ikke lov til å bruke dem til å generere løsninger (enten delvis eller fullstendig) til noen av oppgavene. Funksjoner for automatisk skriving av kode, som Copilot i VS Code, må derfor også være deaktivert mens dere jobber på obligen.  \n",
    "Bruker dere KI-verktøy vil vi også at dere kort beskriver hvordan dere har brukt dem under arbeidet med oppgaven.\n",
    "\n",
    "Det er ikke mulighet for omlevering av obliger som ikke bestås.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b012b",
   "metadata": {},
   "source": [
    "## Oppsett\n",
    "\n",
    "Denne første obligen skal blant annet hjelpe dere med å komme i gang med verktøyene vi kommer til å bruke gjennom semesteret. Sørg for å aktivere IN1160-miljøet, så bør dere kunne kjøre kodeblokken under. Får dere noen feilmeldinger, oppfordres dere til å møte opp i lab- og gruppetimene for teknisk hjelp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4e1c34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sjekker pakker ===\n",
      "[OK] numpy (2.3.2)\n",
      "[OK] matplotlib (3.10.3)\n",
      "[OK] pandas (2.3.1)\n",
      "[OK] ipykernel (6.30.0)\n",
      "[OK] scikit-learn (1.7.1)\n",
      "[OK] seaborn (0.13.2)\n",
      "[OK] openml (0.15.1)\n",
      "[OK] pyyaml (6.0.2)\n",
      "[OK] ipython (9.4.0)\n",
      "[OK] plotly (6.3.1)\n",
      "[OK] gymnasium (1.2.1)\n",
      "[OK] datasets (4.4.1)\n",
      "[OK] gymnasium[toy-text]\n",
      "====================\n",
      "Alt fungerer som det skal!\n"
     ]
    }
   ],
   "source": [
    "from helpers_1a import sanity_check\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648d371",
   "metadata": {},
   "source": [
    "## Bakgrunn\n",
    "\n",
    "I denne innleveringen skal vi jobbe med datasettet _Norsk aviskorpus_ (NAK) fra Nasjonalbiblioteket. Dette korpuset består av tekster hentet fra en rekke norske aviser fra perioden 1998-2019.\n",
    "Det fulle datasettet kan finnes [her](https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-4/).\n",
    "\n",
    "NAK består av over 1 500 000 ord (~2,6 GB), men vi skal jobbe med en forenklet versjon av datasettet, gitt i fila `NAK_oblig_1a.txt`.\n",
    "\n",
    "I denne obligen skal dere jobbe med å lage enkle vektorrepresentasjoner av ord. Dere skal gjøre dette basert på hvilke andre ord de forekommer sammen med i setninger i korpuset. Vi skal så se på hvordan vi kan bruke disse vektorene til å finne likheter mellom ord ved bruk av cosinuslikhet.\n",
    "\n",
    "I oppgavene som følger skal dere:\n",
    "\n",
    "1. gjøre preprosessering av tekstene\n",
    "2. representere ord som vektorer i en vektorrom-modell\n",
    "3. undersøke likhet mellom ord gjennom vektorene\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7295f",
   "metadata": {},
   "source": [
    "## Oppgave 1 – Forberede data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413857b5",
   "metadata": {},
   "source": [
    "### Oppgave 1.1 – Laste inn datasett (1 poeng)\n",
    "\n",
    "I denne oppgaven skal dere laste inn NAK-datasettet fra tekstfila `NAK_oblig_1a.txt`. Dette skal gjøres med funksjonen `load_dataset()`.\n",
    "Denne tar filnavnet til datasettet (som skal ligge i samme mappe som denne notebook-fila) som argument og skal returnere en liste med setningstrenger.\n",
    "\n",
    "Skriv ferdig funksjonen `load_dataset()` i cellen under og last inn datasettet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "96040a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 1.1\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Argumenter:\n",
    "    - dataset_path : En streng som spesifiserer hvor vi finner datasettet.\n",
    "\n",
    "    Returnerer:\n",
    "    - dataset : En liste med setninger.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            dataset.append(line.strip())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c3d06f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bruk load_dataset() til å laste inn datasettet her:\n",
    "loaded_dataset = load_dataset(\"NAK_oblig_1a.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02366171",
   "metadata": {},
   "source": [
    "### Oppgave 1.2 – Tokenisering og inspeksjon av dataene (2 poeng)\n",
    "\n",
    "Vi har nå en liste med setninger, men vi ønsker å utforske de individuelle ordene, så vi må dele opp teksten ytterligere. Denne typen oppdeling av tekst kaller vi for *tokenisering*, fordi hver tekstbit vi mater inn i en modell kalles et *token*. Vi er ofte interessert i å vite hvor mange unike tokens vi har i datasettet vårt. Hvert unike token kaller vi for en *ordtype*, eller bare *type*. Alle ordtypene vi har til sammen kaller vi for *vokabularet* vårt.\n",
    "\n",
    "I følgende setning er det altså 7 tokens og 5 typer (vi teller \"En\" og \"en\" som to forskjellige typer):\n",
    "\n",
    "> En for alle og alle for en\n",
    "\n",
    "I cellen under skal dere lage funksjonen `tokenize()` som tokeniserer en setning ved å dele den opp med metoden `.split()`.\n",
    "\n",
    "Videre skal dere:\n",
    "- Tokenisere hver setning i datasettet med `tokenize()`. \n",
    "  - Dette skal altså resultere i en liste av lister med individuelle ord.\n",
    "- Rapportere hvor mange ord dere får.\n",
    "- Rapportere hvor mange ordtyper dere får.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "903e9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 1.2\n",
    "def tokenize(text):\n",
    "    \"\"\"Tar inn en streng med tekst og returnerer en liste med ord.\"\"\"\n",
    "    #tokens: list[str] = text.split()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "5ea1ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser datasettet her:\n",
    "for i in range(len(loaded_dataset)):\n",
    "    loaded_dataset[i] = tokenize(loaded_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c04bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2b9da6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word/Token count = 28671744\n",
      "\n",
      "Type count = 657933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beregn antall ord og ordtyper her:\n",
    "word_count = 0\n",
    "uniques = set()\n",
    "\n",
    "for i in range(len(loaded_dataset)):\n",
    "    word_count += len(loaded_dataset[i])\n",
    "    uniques.update(loaded_dataset[i])\n",
    "\n",
    "type_count = len(uniques)\n",
    "\n",
    "print(f\"Word/Token count = {word_count}\\n\")\n",
    "print(f\"Type count = {type_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee516be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec51d4f9",
   "metadata": {},
   "source": [
    "### Oppgave 1.3 – Fjerning av stoppord (2 poeng)\n",
    "\n",
    "Hvis alt er gjort riktig så langt bør dere ha fått ~31 000 000 tokens og ~1 000 000 ordtyper. Det betyr altså at hver type i gjennomsnitt forekommer omtrent 31 ganger i datasettet.\n",
    "Vi vet derimot (fra [Zipf's lov](https://en.wikipedia.org/wiki/Zipf%27s_law)) at noen relativt få ord forekommer veldig mange ganger, mens veldig mange andre ord forekommer veldig sjeldent. \n",
    "\n",
    "Blant de mest vanlige ordene finner vi ord som for eksempel \"og\", \"i\" og \"det\". Siden disse ordene forkommer såpass ofte, bidrar de sjeldent med informasjon som hjelper oss med å skille mellom ulike ord. Vi kan derfor hoppe over disse ordene for å redusere kjøretiden på treningen av modellene våre.  \n",
    "Denne prosessen kaller vi å fjerne *stoppord*. En liste med norske stoppord (inkludert tegnsetting) er gitt i funksjonen `get_norwegian_stopwords()` i `helpers_1a.py`.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Hente lista med stoppord med `get_norwegian_stopwords()`. \n",
    "- Fjerne alle stoppordene fra datasettet. \n",
    "  - En setning som *\"en katt hadde spist en mus\"* skal altså reduseres til *\"katt spist mus\"*.\n",
    "- Rapportere hvor mange ordtyper og hvor mange tokens det er igjen i datasettet. \n",
    "  - Skriv en kort kommentar om hvordan forholdet mellom disse tallene endrer seg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c97d594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_1a import get_norwegian_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ae406a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoppord fjernet, Tokens: 1388746\n",
      "Stoppord fjernet, Types: 657930\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 1.3\n",
    "stoppord = get_norwegian_stopwords()\n",
    "\n",
    "for i in range(len(stoppord)):\n",
    "    if stoppord[i] in loaded_dataset[i]:\n",
    "        loaded_dataset[i].remove(stoppord[i])\n",
    "        uniques.discard(stoppord[i])\n",
    "\n",
    "\n",
    "print(f\"Stoppord fjernet, Tokens: {len(loaded_dataset)}\")\n",
    "print(f\"Stoppord fjernet, Types: {len(uniques)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249b691",
   "metadata": {},
   "source": [
    "#### Oppgave 1.3 – Tekstbesvarelse\n",
    "\n",
    "_Hvor mange ordtyper og tokens står dere igjen med etter fjerning av stoppordene?_\n",
    "\n",
    "**Svar**:\n",
    "Etter fjerning av stoppord står jeg igjen med 28 671 744 tokens og 657 933 ordtyper. Sammenlignet med datasettet før stoppordfjerning er antall tokens redusert betydelig, mens antall ordtyper reduseres mindre proposjonalt. Dette skyldes at stoppord som \"og\", \"i\", og \"det\" forekommer svært ofte, men representerer relativt få unike typer. Når disse fjernes, forsvinner mange gjentatte forekomster, noe som senker token-antallet. Forholdet mellom tokens og typer endres dermed slik at hver type i gjennomsnitt forekommer sjeldnere. Dette er ønskelig i vektorrom-modeller, siden de gjenværende ordene typisk bører mer semantisk informasjon og gir et mer informativt representasjonsrom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a07a7",
   "metadata": {},
   "source": [
    "### Oppgave 1.4 – Fjerning av sjeldne ord (2 poeng)\n",
    "\n",
    "Som nevnt er det også vanlig at vi har svært mange ord som forekommer svært sjeldent. Blant disse ordene finner vi ofte feilstavinger og ord som ikke har blitt lest riktig fra teksten.\n",
    "Disse ordene er som oftest heller ikke særlig nyttige for modellene våre og kan derfor også fjernes. \n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Lage en liste med de 10 000 ordene som forekommer oftest i datasettet (der stoppord er fjernet).\n",
    "  - Her kan det være nyttig å benytte seg av klassen `Counter` og metoden `.most_common()`.\n",
    "- Rapportere hvor mange typer og tokens det er i datasettet etter dette.\n",
    "  - Hvordan er forholdet mellom disse nå?\n",
    "\n",
    "Tips: Under utvikling kan det være lurt å begrense vokabularet enda litt mer. Dere kan f.eks. hente de 100 vanligste ordene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ee018b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "4c25f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word/Token count = 24368510\n",
      "\n",
      "Type count = 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 1.4\n",
    "all_tokens = []\n",
    "for sentence in loaded_dataset:\n",
    "    all_tokens.extend(sentence)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "dataset_10k = counter.most_common(10000)\n",
    "top_10k_words = {word for word, freq in dataset_10k}\n",
    "\n",
    "filter_dataset = []\n",
    "for sentence in loaded_dataset:\n",
    "    filtered_sentence = [word for word in sentence if word in top_10k_words]\n",
    "    filter_dataset.append(filtered_sentence)\n",
    "\n",
    "word_count = 0\n",
    "uniques = set()\n",
    "\n",
    "for sentence in filter_dataset:\n",
    "    word_count += len(sentence)\n",
    "    uniques.update(sentence)\n",
    "\n",
    "print(f\"Word/Token count = {word_count}\\n\")\n",
    "print(f\"Type count = {len(uniques)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74d3ad",
   "metadata": {},
   "source": [
    "#### Oppgave 1.4 – Tekstbesvarelse\n",
    "\n",
    "_Hvor mange typer og tokens står dere igjen med etter fjerning av de sjeldne ordene?_\n",
    "\n",
    "**Svar:**\n",
    "Etter fjerning av sjeldne ord ved å begrense vokabulæret til de 10 000 mest frekvente ordene i datasettet, er det 10 000 resterende ordtyper. Antall tokens reduseres betydelig sammenlignet med datasettet før filtrering, siden alle sjelde ord og feilstavinger fjernes fullstendig. Forholdet mellom tokens og typer endres dermed slik at hver type i gjennomsnitt forekommer langt oftere enn tidligere. Dette gir et mer kompakt og informativt datasett, der ord som faktisk bidrar til semantisk struktur dominerer. En slik reduksjon er gunstig både for kjøretid og for kvaliteten på vektorrepresentasjonene som senere skal trenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255ff66",
   "metadata": {},
   "source": [
    "## Oppgave 2 – Vektorisering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25651f6",
   "metadata": {},
   "source": [
    "### Oppgave 2.1 – Fra ord til tall (2 poeng)\n",
    "\n",
    "\n",
    "Vi har nå fjernet både de vanligste og sjeldneste de ordene fra vokabularet vårt. De resterende ordene burde være en passe mengde for modellen vår.\n",
    "Modellen vi skal bruke kan derimot ikke bruke strenger som treningsdata, så vi må lage tallrepresentasjoner av dem, der hver type i vokabularet vårt knyttes til et bestemt tall.\n",
    "\n",
    "For å holde styr på sammenhengen mellom ordene og tall kan vi bruke en ordbok der vi tilordner et tall til hvert ord. For å knytte tallene tilbake til ordene kan vi lage en tilsvarende ordbok med tallet som nøkkel og ordet som verdi.\n",
    "\n",
    "I denne oppgaven skal dere fylle ut de to ordbøkene `word_to_int` og `int_to_word`. Her kan funksjonen `enumerate()` være nyttig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "94e1e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.1\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1ef7",
   "metadata": {},
   "source": [
    "### Oppgave 2.2 – Bygging av vektorer (5 poeng)\n",
    "\n",
    "I denne oppgaven skal vi lage vektorrepresentasjoner av ordene vi kom fram til i **1.4**. Vi skal benytte oss av enkle tellevektorer, der vi bruker _konteksten_ et ord befinner seg i til å informere oss om hva slags ord det er snakk om. Intuisjonen for dette er at ord som brukes i samme kontekst typisk har like eller relaterte betydninger:\n",
    "\n",
    "> \"_You shall know a word by the company it keeps_\"  \n",
    ">  – J. R. Firth (1957)\n",
    "\n",
    "I cellen under finner dere en påbegynt `CountVectorizer`-klasse.\n",
    "Når det blir opprettet en instans av denne, lager metoden `zeros()` en matrise av nuller som er like lang og like bred som vokabularet vi sender inn. Her representerer hver rad et ord i vokabularet vårt, der f.eks. rad 5 representerer ordet som ble tilordnet tallet 5 i forrige oppgave. Kolonnene fungerer på samme måte. \n",
    "\n",
    "Etter å ha opprettet en `CountVectorizer`, skal man kunne kalle på metoden `.fit()`, der man sender inn de tokeniserte tekstdataene fra oppgave **1.2**. Denne sender én og én setning inn til metoden `_add_sentence()`, som dere skal fylle ut.  \n",
    "Denne metoden skal oppdatere matrisen ved å telle antall ganger to ord forekommer i samme setning. For hvert ord $i$ i setningen, skal vi altså legge til 1 på posisjon $(i, j)$ i matrisen for hvert ord $j$ som også forekommer i setningen.\n",
    "\n",
    "Når treningen er overstått kan vi hente ut en vektor for et ord ved å bruke metoden `get_vector()`.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Fylle ut metoden `_add_sentence()` slik at matrisen fylles ut på riktig måte.\n",
    "- Opprette en instans av `CountVectorizer`.\n",
    "- Trene denne på de tokeniserte dataene.\n",
    "  - Dette kan ta litt tid avhengig av maskinen dere kjører på.\n",
    "- Hente ut en vektor for et ord dere selv velger. Finn ut av hvilket annet ord dette forekommer oftest med.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "484fa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, vocab, word_to_int):\n",
    "        self.vocab = vocab\n",
    "        self.word_to_int = word_to_int\n",
    "        self.matrix = self._create_matrix(vocab)\n",
    "\n",
    "    def _create_matrix(self, vocab):\n",
    "        \"\"\"\n",
    "        Lager en matrise bestående av 0-vektorer. Matrisen er\n",
    "        like lang og like bred som vokabularet vårt, slik at\n",
    "        hver rad representerer et ord, og hver kolonne\n",
    "        representerer antall ganger et annet ord forekommer\n",
    "        med rad-ordet.\n",
    "        \"\"\"\n",
    "        matrix = zeros((len(vocab), len(vocab)))\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _add_sentence(self, sentence_tokens):\n",
    "        \"\"\"\n",
    "        Oppdaterer matrisen med observasjoner fra setningen.\n",
    "        \"\"\"\n",
    "        # Fyll ut denne metoden!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Trener modellen på `dataset`.\"\"\"\n",
    "        for sent in dataset:\n",
    "            self._add_sentence(sent)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Henter vektoren for et gitt ord.\"\"\"\n",
    "        word_int = self.word_to_int[word]\n",
    "\n",
    "        return self.matrix[word_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "53ef6997",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CountVectorizer.__init__() missing 2 required positional arguments: 'vocab' and 'word_to_int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[313]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Oppgave 2.2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Lag en instans av CountVectorizer med våre parametre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m vectorizer = \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Tren vektorisereren på våre data\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: CountVectorizer.__init__() missing 2 required positional arguments: 'vocab' and 'word_to_int'"
     ]
    }
   ],
   "source": [
    "# Oppgave 2.2\n",
    "# Lag en instans av CountVectorizer med våre parametre\n",
    "vectorizer = CountVectorizer()\n",
    "# Tren vektorisereren på våre data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.2\n",
    "# Hent en vektor for et ord og finn ut av hvilket ord det oftest forekommer sammen med.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044739a",
   "metadata": {},
   "source": [
    "### Oppgave 2.3 – Vektordimensjonalitet (2 poeng)\n",
    "\n",
    "Hva kan dere si om vektorene dere nå har laget? Er de høydimensjonale eller ikke? Nevn minst en fordel og en ulempe ved denne typen vektorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1404d",
   "metadata": {},
   "source": [
    "#### Oppgave 2.3 – Tekstbesvarelse\n",
    "\n",
    "_Svar her._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4002c9",
   "metadata": {},
   "source": [
    "### Oppgave 2.4 – Måle distanser med cosinuslikhet (4 poeng)\n",
    "\n",
    "Vi har nå laget en vektorrommodell av ordene i datasettet vårt, der hver kolonne representerer et ord i vokabularet vårt. Vi skal nå forsøke å tallfeste hvor like to vektorer, så vi kan se om dette stemmer overens med likheter mellom ord.\n",
    "Dette skal vi gjøre ved bruk av cosinuslikhet (engelsk: _cosine similarity_), der vi måler vinkelen mellom to vektorer.\n",
    "\n",
    "Cosinuslikheten mellom to vektorer $a$ og $b$ er definert som:\n",
    "\n",
    "$$\n",
    "cos(a,\\ b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "$$\n",
    "\n",
    "Her er $a \\cdot b$ prikkproduktet av de to vektorene, og $\\|a\\|$ og $\\|b\\|$ er normen (lengden) til henholdsvis vektor $a$ og $b$.\n",
    "\n",
    "Dere skal implementere funksjonen for cosinuslikhet på egenhånd. Det vil si at dere ikke skal importere ferdige funksjoner, men heller implementere dette direkte i Python.\n",
    "Dere kan derimot bruke pakken `math`, som har en rekke nyttige funksjoner for numeriske beregninger.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Implementere funksjonen `cosine_similarity(a, b)` som tar inn to vektorer og returnerer cosinuslikheten.\n",
    "- Bruke funksjonen til å finne cosinuslikheten mellom to ord dere selv velger. Skriv en kort kommentar på hva dere observerer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.4\n",
    "import math\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    # Regn ut cosinuslikhet her\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f46cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksempelbruk:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724af4f",
   "metadata": {},
   "source": [
    "### Oppgave 2.5 – Finne likheter mellom ord (5 poeng)\n",
    "\n",
    "Vi skal nå bruke cosinuslikhet til å finne ord som er like hverandre i vektorrommet vårt.\n",
    "I cellen under finner dere to sett med ord i listene `WORDS` og `WORDS_SUBSET`. For hvert av ordene i `WORDS_SUBSET` skal dere finne de fem ordene fra `WORDS` som er likest, målt med cosinuslikhet.  \n",
    "Lag en tabell i en markdown-celle der dere setter inn hvert ord i en kolonne og de fem likeste ordene i en annen kolonne. \n",
    "Dette kan dere gjøre på følgende måte:\n",
    "\n",
    "```md\n",
    "| Ord    | 5 likeste |\n",
    "| ------ | --------- |\n",
    "| hund   | ...       |\n",
    "| salat  | ...       |\n",
    "| bil    | ...       |\n",
    "...\n",
    "```\n",
    "\n",
    "Diskuter kort resultatene dere får. Virker det som at likere ord får høyere cosinuslikhet? Kom med noen eksempler og/eller moteksempler.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Regne ut cosinuslikhet mellom hvert ord i `WORDS_SUBSET` og alle ordene i `WORDS`.\n",
    "- Lage en tabell med de fem likeste ordene for hvert ord. \n",
    "- Diskutere resultatene deres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = [\n",
    "    \"gris\", \"elefant\", \"hund\", \"katt\", \"hest\", \"bjørn\", \"kylling\", \"svin\",\n",
    "    \"fisk\", \"salat\", \"brød\", \"ost\", \"smør\", \"melk\", \"bil\", \"moped\", \"sykkel\",\n",
    "    \"båt\", \"fly\", \"tog\", \"sykepleier\", \"lege\", \"ingeniør\", \"sjef\",\n",
    "    \"sjåfør\", \"designer\", \"jente\", \"gutt\", \"mann\", \"kvinne\", \"Oslo\", \"Trondheim\",\n",
    "    \"Roma\", \"Egypt\", \"Mexico\", \"Galdhøpiggen\", \"Sørlandet\", \"Sahara\", \"Bjørn\",\n",
    "    \"Anne\", \"Jonas\", \"Erna\", \"Donald\", \"Barack\", \"Mohamed\", \"Angela\"\n",
    "]\n",
    "WORDS_SUBSET = [\n",
    "    \"hund\", \"salat\", \"bil\", \"ingeniør\", \"sjef\", \"jente\", \"Oslo\", \"Jonas\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947277d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7d9da",
   "metadata": {},
   "source": [
    "#### Oppgave 2.5 – Tekstbesvarelse\n",
    "\n",
    "_Legg inn tabellen din her. Hva ser du av resultatene?_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391aef09",
   "metadata": {},
   "source": [
    "## NB!\n",
    "\n",
    "Husk å kjøre gjennom hele notebooken fra toppen av før dere leverer obligen! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
