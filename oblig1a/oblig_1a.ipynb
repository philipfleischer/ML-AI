{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f792e338",
   "metadata": {},
   "source": [
    "# IN1160 – Oblig 1a: Vektorrom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730524db",
   "metadata": {},
   "source": [
    "**Våren 2026**\n",
    "\n",
    "Det er en god idé å lese gjennom hele oppgavesettet før dere setter i gang.\n",
    "Dersom dere har spørsmål så kan dere:\n",
    "\n",
    "- spørre i gruppetimene,\n",
    "- spørre på Discourse\n",
    "- eller sende epost til in1160-support@ifi.uio.no dersom alternativene over av en eller annen grunn ikke passer for spørsmålet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902bec1",
   "metadata": {},
   "source": [
    "## Innlevering\n",
    "\n",
    "Oppgaven leveres innen 11.02. klokken 23:59 i [Devilry](https://devilry.ifi.uio.no/). \n",
    "\n",
    "Innleveringen skal bestå av én Jupyter notebook med både kode og tilhørende forklaringer. **Før innlevering skal du kjøre gjennom hele notebooken, før du lagrer siste gang. Den bør kjøre uten å feile og vise den grafikken og de utskriftene som skal være med.**\n",
    "Skulle dere av tekniske grunner ikke kunne besvare alle spørsmålene i en notebook kan dere i denne obligen også levere vanlige Python-filer, sammen med en PDF som inneholder grafikker og resultater fra kjøring, samt deres tekst-besvarelser. Ta kontakt med emnestaben, så vi får ordnet eventuelle problemer før oblig 1b.\n",
    "\n",
    "Vi understreker at innlevering av kode alene ikke er nok for å bestå oppgaven – vi forventer at notebooken også skal inneholde kommentarer (på norsk eller engelsk) på hva dere har gjort og begrunnelser for valgene dere har tatt underveis.\n",
    "La enhver oblig bli en trening i å formidle forskning. Bruk helst hele setninger, og matematiske formler om nødvendig. Resultater skal presenteres i tabeller på en oversiktlig måte.\n",
    "Det å forklare med egne ord, bruke begreper vi har gått gjennom på forelesningene og å forklare og reflektere over løsningene deres er en viktig del av læringsprosessen – ta det på alvor!\n",
    "\n",
    "Når det gjelder bruk av generative prateroboter (ChatGPT og lignende): Dere kan bruke dem som en \"sparringspartner\", for eksempel for å forklare noe dere ikke helt har forstått. Dere har imidlertid ikke lov til å bruke dem til å generere løsninger (enten delvis eller fullstendig) til noen av oppgavene. Funksjoner for automatisk skriving av kode, som Copilot i VS Code, må derfor også være deaktivert mens dere jobber på obligen.  \n",
    "Bruker dere KI-verktøy vil vi også at dere kort beskriver hvordan dere har brukt dem under arbeidet med oppgaven.\n",
    "\n",
    "Det er ikke mulighet for omlevering av obliger som ikke bestås.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b012b",
   "metadata": {},
   "source": [
    "## Oppsett\n",
    "\n",
    "Denne første obligen skal blant annet hjelpe dere med å komme i gang med verktøyene vi kommer til å bruke gjennom semesteret. Sørg for å aktivere IN1160-miljøet, så bør dere kunne kjøre kodeblokken under. Får dere noen feilmeldinger, oppfordres dere til å møte opp i lab- og gruppetimene for teknisk hjelp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1c34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sjekker pakker ===\n",
      "[OK] numpy (2.3.2)\n",
      "[OK] matplotlib (3.10.3)\n",
      "[OK] pandas (2.3.1)\n",
      "[OK] ipykernel (6.30.0)\n",
      "[OK] scikit-learn (1.7.1)\n",
      "[OK] seaborn (0.13.2)\n",
      "[OK] openml (0.15.1)\n",
      "[OK] pyyaml (6.0.2)\n",
      "[OK] ipython (9.4.0)\n",
      "[OK] plotly (6.3.1)\n",
      "[OK] gymnasium (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruker/Desktop/Projects/ML-AI/mlai_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] datasets (4.4.1)\n",
      "[OK] gymnasium[toy-text]\n",
      "====================\n",
      "Alt fungerer som det skal!\n"
     ]
    }
   ],
   "source": [
    "from helpers_1a import sanity_check\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648d371",
   "metadata": {},
   "source": [
    "## Bakgrunn\n",
    "\n",
    "I denne innleveringen skal vi jobbe med datasettet _Norsk aviskorpus_ (NAK) fra Nasjonalbiblioteket. Dette korpuset består av tekster hentet fra en rekke norske aviser fra perioden 1998-2019.\n",
    "Det fulle datasettet kan finnes [her](https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-4/).\n",
    "\n",
    "NAK består av over 1 500 000 ord (~2,6 GB), men vi skal jobbe med en forenklet versjon av datasettet, gitt i fila `NAK_oblig_1a.txt`.\n",
    "\n",
    "I denne obligen skal dere jobbe med å lage enkle vektorrepresentasjoner av ord. Dere skal gjøre dette basert på hvilke andre ord de forekommer sammen med i setninger i korpuset. Vi skal så se på hvordan vi kan bruke disse vektorene til å finne likheter mellom ord ved bruk av cosinuslikhet.\n",
    "\n",
    "I oppgavene som følger skal dere:\n",
    "\n",
    "1. gjøre preprosessering av tekstene\n",
    "2. representere ord som vektorer i en vektorrom-modell\n",
    "3. undersøke likhet mellom ord gjennom vektorene\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7295f",
   "metadata": {},
   "source": [
    "## Oppgave 1 – Forberede data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413857b5",
   "metadata": {},
   "source": [
    "### Oppgave 1.1 – Laste inn datasett (1 poeng)\n",
    "\n",
    "I denne oppgaven skal dere laste inn NAK-datasettet fra tekstfila `NAK_oblig_1a.txt`. Dette skal gjøres med funksjonen `load_dataset()`.\n",
    "Denne tar filnavnet til datasettet (som skal ligge i samme mappe som denne notebook-fila) som argument og skal returnere en liste med setningstrenger.\n",
    "\n",
    "Skriv ferdig funksjonen `load_dataset()` i cellen under og last inn datasettet:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193ed6c",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 1.1**\n",
    "Jeg laster inn NAK-datasettet fra en tekstfil som blir gitt av å åpne .zip mappen. Filen leses linje for line og linjene blir lagret som setninger i en liste. Bruker stip() for å fjerne linjeskift og eventuelle mellomrom før og etter, fordi tokeniseringen kan bli påvirket av dette, dette er best practice. load_dataset() funksjonen resulterer i en liste av setningsstrenger, der hvert element representerer en linje i datasettet, som gjør det svært enkelt å iterere over datasettet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96040a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 1.1\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Argumenter:\n",
    "    - dataset_path : En streng som spesifiserer hvor vi finner datasettet.\n",
    "\n",
    "    Returnerer:\n",
    "    - dataset : En liste med setninger.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            dataset.append(line.strip())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d06f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bruk load_dataset() til å laste inn datasettet her:\n",
    "loaded_dataset = load_dataset(\"NAK_oblig_1a.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02366171",
   "metadata": {},
   "source": [
    "### Oppgave 1.2 – Tokenisering og inspeksjon av dataene (2 poeng)\n",
    "\n",
    "Vi har nå en liste med setninger, men vi ønsker å utforske de individuelle ordene, så vi må dele opp teksten ytterligere. Denne typen oppdeling av tekst kaller vi for *tokenisering*, fordi hver tekstbit vi mater inn i en modell kalles et *token*. Vi er ofte interessert i å vite hvor mange unike tokens vi har i datasettet vårt. Hvert unike token kaller vi for en *ordtype*, eller bare *type*. Alle ordtypene vi har til sammen kaller vi for *vokabularet* vårt.\n",
    "\n",
    "I følgende setning er det altså 7 tokens og 5 typer (vi teller \"En\" og \"en\" som to forskjellige typer):\n",
    "\n",
    "> En for alle og alle for en\n",
    "\n",
    "I cellen under skal dere lage funksjonen `tokenize()` som tokeniserer en setning ved å dele den opp med metoden `.split()`.\n",
    "\n",
    "Videre skal dere:\n",
    "- Tokenisere hver setning i datasettet med `tokenize()`. \n",
    "  - Dette skal altså resultere i en liste av lister med individuelle ord.\n",
    "- Rapportere hvor mange ord dere får.\n",
    "- Rapportere hvor mange ordtyper dere får.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc415140",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 1.2**\n",
    "I denne oppgaven tokeniseres setningene ved bruk av metoden .split(), som deler opp setningens ord på whitespaces.\n",
    "Datasettet resulterer nå i en liste av lister, der hver indre liste består av tokens/ord. Deretter beregner jeg antall tokens og antall ordtyper. Tokens -> lengen av hver setning summert sammen. Antall ordtyper -> lengden av tokens i et set(), der kun unike elementer forekommer.\n",
    "\"En\" og \"en\" behandles som unike tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903e9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 1.2\n",
    "def tokenize(text):\n",
    "    \"\"\"Tar inn en streng med tekst og returnerer en liste med ord.\"\"\"\n",
    "    #tokens: list[str] = text.split()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea1ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser datasettet her:\n",
    "for i in range(len(loaded_dataset)):\n",
    "    loaded_dataset[i] = tokenize(loaded_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9da6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word/Token count = 28671744\n",
      "\n",
      "Type count = 657933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beregn antall ord og ordtyper her:\n",
    "word_count = 0\n",
    "uniques = set()\n",
    "\n",
    "for i in range(len(loaded_dataset)):\n",
    "    word_count += len(loaded_dataset[i])\n",
    "    uniques.update(loaded_dataset[i])\n",
    "\n",
    "type_count = len(uniques)\n",
    "\n",
    "print(f\"Word/Token count = {word_count}\\n\")\n",
    "print(f\"Type count = {type_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51d4f9",
   "metadata": {},
   "source": [
    "### Oppgave 1.3 – Fjerning av stoppord (2 poeng)\n",
    "\n",
    "Hvis alt er gjort riktig så langt bør dere ha fått ~31 000 000 tokens og ~1 000 000 ordtyper. Det betyr altså at hver type i gjennomsnitt forekommer omtrent 31 ganger i datasettet.\n",
    "Vi vet derimot (fra [Zipf's lov](https://en.wikipedia.org/wiki/Zipf%27s_law)) at noen relativt få ord forekommer veldig mange ganger, mens veldig mange andre ord forekommer veldig sjeldent. \n",
    "\n",
    "Blant de mest vanlige ordene finner vi ord som for eksempel \"og\", \"i\" og \"det\". Siden disse ordene forkommer såpass ofte, bidrar de sjeldent med informasjon som hjelper oss med å skille mellom ulike ord. Vi kan derfor hoppe over disse ordene for å redusere kjøretiden på treningen av modellene våre.  \n",
    "Denne prosessen kaller vi å fjerne *stoppord*. En liste med norske stoppord (inkludert tegnsetting) er gitt i funksjonen `get_norwegian_stopwords()` i `helpers_1a.py`.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Hente lista med stoppord med `get_norwegian_stopwords()`. \n",
    "- Fjerne alle stoppordene fra datasettet. \n",
    "  - En setning som *\"en katt hadde spist en mus\"* skal altså reduseres til *\"katt spist mus\"*.\n",
    "- Rapportere hvor mange ordtyper og hvor mange tokens det er igjen i datasettet. \n",
    "  - Skriv en kort kommentar om hvordan forholdet mellom disse tallene endrer seg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed2744",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 1.3**\n",
    "Fjerner stoppord fra datasettet ved å bruke gisten gitt av get_norwegian_stopwords. Hensikten med dette er å redusere støyen som forekommer og i tillegg gjøre programmet mer effektivt ved å ikke behandle hyppige ord som \"og\", \"i\", og lignende.\n",
    "Antall tokens synker betraktelig etter filtreringen av stopp-ordene, antall ordtyper synker også, men ikke like mye.\n",
    "Dette resulterer i at frekvensen mellom hver type blir høyere siden vi fjerner mange av de hyppige ordene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97d594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_1a import get_norwegian_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae406a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoppord fjernet, Tokens: 1388746\n",
      "Stoppord fjernet, Types: 657930\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 1.3\n",
    "stoppord = get_norwegian_stopwords()\n",
    "\n",
    "for i in range(len(stoppord)):\n",
    "    if stoppord[i] in loaded_dataset[i]:\n",
    "        loaded_dataset[i].remove(stoppord[i])\n",
    "        uniques.discard(stoppord[i])\n",
    "\n",
    "\n",
    "print(f\"Stoppord fjernet, Tokens: {len(loaded_dataset)}\")\n",
    "print(f\"Stoppord fjernet, Types: {len(uniques)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249b691",
   "metadata": {},
   "source": [
    "#### Oppgave 1.3 – Tekstbesvarelse\n",
    "\n",
    "_Hvor mange ordtyper og tokens står dere igjen med etter fjerning av stoppordene?_\n",
    "\n",
    "**Svar**:\n",
    "Etter fjerning av stoppord står jeg igjen med 28 671 744 tokens og 657 933 ordtyper. Sammenlignet med datasettet før stoppordfjerning er antall tokens redusert betydelig, mens antall ordtyper reduseres mindre proposjonalt. Dette skyldes at stoppord som \"og\", \"i\", og \"det\" forekommer svært ofte, men representerer relativt få unike typer. Når disse fjernes, forsvinner mange gjentatte forekomster, noe som senker token-antallet. Forholdet mellom tokens og typer endres dermed slik at hver type i gjennomsnitt forekommer sjeldnere. Dette er ønskelig i vektorrom-modeller, siden de gjenværende ordene typisk bører mer semantisk informasjon og gir et mer informativt representasjonsrom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a07a7",
   "metadata": {},
   "source": [
    "### Oppgave 1.4 – Fjerning av sjeldne ord (2 poeng)\n",
    "\n",
    "Som nevnt er det også vanlig at vi har svært mange ord som forekommer svært sjeldent. Blant disse ordene finner vi ofte feilstavinger og ord som ikke har blitt lest riktig fra teksten.\n",
    "Disse ordene er som oftest heller ikke særlig nyttige for modellene våre og kan derfor også fjernes. \n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Lage en liste med de 10 000 ordene som forekommer oftest i datasettet (der stoppord er fjernet).\n",
    "  - Her kan det være nyttig å benytte seg av klassen `Counter` og metoden `.most_common()`.\n",
    "- Rapportere hvor mange typer og tokens det er i datasettet etter dette.\n",
    "  - Hvordan er forholdet mellom disse nå?\n",
    "\n",
    "Tips: Under utvikling kan det være lurt å begrense vokabularet enda litt mer. Dere kan f.eks. hente de 100 vanligste ordene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d177ab8",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 1.4**\n",
    "I denne oppgaven lages det en frekvensteller over tokens-ene i datasettet ved å samle dem i en ny tom liste og bruke Counter funksjonen. \n",
    "Deretter finner jeg de 10 000 vanligste ordene med funksjonen most_common(100000) og datasettet blir dermed filtrert slik at hvert ord ikke funnet i denne listen blir fjernet, siden vi ikke er like interessert i dem.\n",
    "Grunnen til at interessen synker for disse ordene handler om at de har en høyere sannsynlighet for at inneholder feilstavinger og andre feil/defekter. Samtidig så vil de være \"outliers\" som ikke gir en like sterk statistisk begrunnelse for at skal bli inkludert i telelrbaserte modeller. \n",
    "Fordelene er rett og slett at vi reduserer kjøretiden og minnebruken betydelig ved å redusere til 10 000 og vi antar at vi får et godt nok representativt vokabulær."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee018b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c25f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word/Token count = 24368510\n",
      "\n",
      "Type count = 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 1.4\n",
    "all_tokens = []\n",
    "for sentence in loaded_dataset:\n",
    "    all_tokens.extend(sentence)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "dataset_10k = counter.most_common(10000)\n",
    "top_10k_words = {word for word, freq in dataset_10k}\n",
    "\n",
    "filter_dataset = []\n",
    "for sentence in loaded_dataset:\n",
    "    filtered_sentence = [word for word in sentence if word in top_10k_words]\n",
    "    filter_dataset.append(filtered_sentence)\n",
    "\n",
    "word_count = 0\n",
    "uniques = set()\n",
    "\n",
    "for sentence in filter_dataset:\n",
    "    word_count += len(sentence)\n",
    "    uniques.update(sentence)\n",
    "\n",
    "print(f\"Word/Token count = {word_count}\\n\")\n",
    "print(f\"Type count = {len(uniques)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74d3ad",
   "metadata": {},
   "source": [
    "#### Oppgave 1.4 – Tekstbesvarelse\n",
    "\n",
    "_Hvor mange typer og tokens står dere igjen med etter fjerning av de sjeldne ordene?_\n",
    "\n",
    "**Svar:**\n",
    "Etter fjerning av sjeldne ord ved å begrense vokabulæret til de 10 000 mest frekvente ordene i datasettet, er det 10 000 resterende ordtyper. Antall tokens reduseres betydelig sammenlignet med datasettet før filtrering, siden alle sjelde ord og feilstavinger fjernes fullstendig. Forholdet mellom tokens og typer endres dermed slik at hver type i gjennomsnitt forekommer langt oftere enn tidligere. Dette gir et mer kompakt og informativt datasett, der ord som faktisk bidrar til semantisk struktur dominerer. En slik reduksjon er gunstig både for kjøretid og for kvaliteten på vektorrepresentasjonene som senere skal trenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255ff66",
   "metadata": {},
   "source": [
    "## Oppgave 2 – Vektorisering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25651f6",
   "metadata": {},
   "source": [
    "### Oppgave 2.1 – Fra ord til tall (2 poeng)\n",
    "\n",
    "\n",
    "Vi har nå fjernet både de vanligste og sjeldneste de ordene fra vokabularet vårt. De resterende ordene burde være en passe mengde for modellen vår.\n",
    "Modellen vi skal bruke kan derimot ikke bruke strenger som treningsdata, så vi må lage tallrepresentasjoner av dem, der hver type i vokabularet vårt knyttes til et bestemt tall.\n",
    "\n",
    "For å holde styr på sammenhengen mellom ordene og tall kan vi bruke en ordbok der vi tilordner et tall til hvert ord. For å knytte tallene tilbake til ordene kan vi lage en tilsvarende ordbok med tallet som nøkkel og ordet som verdi.\n",
    "\n",
    "I denne oppgaven skal dere fylle ut de to ordbøkene `word_to_int` og `int_to_word`. Her kan funksjonen `enumerate()` være nyttig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b8c2c",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 2.1**\n",
    "For å representere ord i en matrise så må jeg først mappe hver ordtype til et heltall. På grunn av dette, så lager jeg en sortert vokabulærliste og bruker funksjonen enumerate() til å lage to ordbøker, nemlig: word_to_int, ord->idx og int_to_word, idx->ord.\n",
    "Ved å mappe dem begge veier slik som nevnt, så kan man effektivisere og muliggjøre rad og kolonneoppslag for ønsket resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e1e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.1\n",
    "vocab = sorted(uniques)\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "\n",
    "for idx, word in enumerate(vocab):\n",
    "    word_to_int[word] = idx\n",
    "    int_to_word[idx] = word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1ef7",
   "metadata": {},
   "source": [
    "### Oppgave 2.2 – Bygging av vektorer (5 poeng)\n",
    "\n",
    "I denne oppgaven skal vi lage vektorrepresentasjoner av ordene vi kom fram til i **1.4**. Vi skal benytte oss av enkle tellevektorer, der vi bruker _konteksten_ et ord befinner seg i til å informere oss om hva slags ord det er snakk om. Intuisjonen for dette er at ord som brukes i samme kontekst typisk har like eller relaterte betydninger:\n",
    "\n",
    "> \"_You shall know a word by the company it keeps_\"  \n",
    ">  – J. R. Firth (1957)\n",
    "\n",
    "I cellen under finner dere en påbegynt `CountVectorizer`-klasse.\n",
    "Når det blir opprettet en instans av denne, lager metoden `zeros()` en matrise av nuller som er like lang og like bred som vokabularet vi sender inn. Her representerer hver rad et ord i vokabularet vårt, der f.eks. rad 5 representerer ordet som ble tilordnet tallet 5 i forrige oppgave. Kolonnene fungerer på samme måte. \n",
    "\n",
    "Etter å ha opprettet en `CountVectorizer`, skal man kunne kalle på metoden `.fit()`, der man sender inn de tokeniserte tekstdataene fra oppgave **1.2**. Denne sender én og én setning inn til metoden `_add_sentence()`, som dere skal fylle ut.  \n",
    "Denne metoden skal oppdatere matrisen ved å telle antall ganger to ord forekommer i samme setning. For hvert ord $i$ i setningen, skal vi altså legge til 1 på posisjon $(i, j)$ i matrisen for hvert ord $j$ som også forekommer i setningen.\n",
    "\n",
    "Når treningen er overstått kan vi hente ut en vektor for et ord ved å bruke metoden `get_vector()`.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Fylle ut metoden `_add_sentence()` slik at matrisen fylles ut på riktig måte.\n",
    "- Opprette en instans av `CountVectorizer`.\n",
    "- Trene denne på de tokeniserte dataene.\n",
    "  - Dette kan ta litt tid avhengig av maskinen dere kjører på.\n",
    "- Hente ut en vektor for et ord dere selv velger. Finn ut av hvilket annet ord dette forekommer oftest med.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26743c81",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Del 1/3 Oppgave 2.2**\n",
    "Implementert metoden _add_sentence(), som fyller samforekomstmatrisen basert på setningene i datasettet. Hvert ord blir først mappet til en indeks ved hjelp av word_to_int listen. Teller deretter hvor ofte ordpar forekommer i setningen og bruker NumPy indekser i stedet for en dobbel løkke, for å gjøre det nært C-kode og dermed veldig effektivt. Jeg trekker i fra diagonalen slik at ord ikke telles med seg selv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "484fa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "import numpy as np\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, vocab, word_to_int):\n",
    "        self.vocab = vocab\n",
    "        self.word_to_int = word_to_int\n",
    "        self.matrix = self._create_matrix(vocab)\n",
    "\n",
    "    def _create_matrix(self, vocab):\n",
    "        \"\"\"\n",
    "        Lager en matrise bestående av 0-vektorer. Matrisen er\n",
    "        like lang og like bred som vokabularet vårt, slik at\n",
    "        hver rad representerer et ord, og hver kolonne\n",
    "        representerer antall ganger et annet ord forekommer\n",
    "        med rad-ordet.\n",
    "        \"\"\"\n",
    "        matrix = zeros((len(vocab), len(vocab)), dtype=np.int32)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _add_sentence(self, sentence_tokens):\n",
    "        \"\"\"\n",
    "        Oppdaterer matrisen med observasjoner fra setningen.\n",
    "        \"\"\"\n",
    "        # Bruker np.array i stedet for dobbel løkke, int32 -> Effektivt\n",
    "        idxs = np.array([self.word_to_int[word] for word in sentence_tokens], dtype=np.int32)\n",
    "        # Legger til 1 for alle par i setningen\n",
    "        self.matrix[np.ix_(idxs, idxs)] += 1\n",
    "        # Fjerner like (co occurrence)\n",
    "        self.matrix[idxs, idxs] -= 1\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Trener modellen på `dataset`.\"\"\"\n",
    "        for sent in dataset:\n",
    "            self._add_sentence(sent)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Henter vektoren for et gitt ord.\"\"\"\n",
    "        word_int = self.word_to_int[word]\n",
    "\n",
    "        return self.matrix[word_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99879f42",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Del 2/3 Oppgave 2.2**\n",
    "Etter vektoriseringen, trenes modellen ved bruk av funksjonen fit() på datasettet fra oppgave 1.4. Setningene itereres og sendes til _add_sentence() funksjonen.\n",
    "Treningen er dermed å danne samforekomstmatrisen. Hver rad i matrisen resulterer i en tellevektor som beskriver konteksten ordet opptrer i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ef6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.2\n",
    "# Lag en instans av CountVectorizer med våre parametre\n",
    "vectorizer = CountVectorizer(vocab, word_to_int)\n",
    "vectorizer.fit(filter_dataset)\n",
    "# Tren vektorisereren på våre data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1183db",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Del 3/3 Oppgave 2.2**\n",
    "Når modellen er ferdig trent, så hentes vektoren til et ord av get_vector(). argmax() blir brukt for å finne det største vektoret. Slår deretter opp i int_to_word for å finne ordet som forekommer flest ganger sammen med målordet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b663b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ord: 26\n",
      "Forekommer oftest med: i\n",
      "Antall ganger: 3545\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 2.2\n",
    "# Hent en vektor for et ord og finn ut av hvilket ord det oftest forekommer sammen med.\n",
    "import numpy as np\n",
    "word = \"26\"\n",
    "vec = vectorizer.get_vector(word)\n",
    "\n",
    "best_j = int(np.argmax(vec))\n",
    "most_common_word = int_to_word[best_j]\n",
    "\n",
    "print(\"Ord:\", word)\n",
    "print(\"Forekommer oftest med:\", most_common_word)\n",
    "print(\"Antall ganger:\", vec[best_j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044739a",
   "metadata": {},
   "source": [
    "### Oppgave 2.3 – Vektordimensjonalitet (2 poeng)\n",
    "\n",
    "Hva kan dere si om vektorene dere nå har laget? Er de høydimensjonale eller ikke? Nevn minst en fordel og en ulempe ved denne typen vektorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1404d",
   "metadata": {},
   "source": [
    "#### Oppgave 2.3 – Tekstbesvarelse\n",
    "\n",
    "**Svar:**\n",
    "Vektorene vi har laget er høydimensjonale, siden hver vektor har én dimensjon per ord i vokabulæret ( rundt 10 000 dimensjoner). Hver dimensjon representerer hvor ofte et annet ord forekommer i samme setning, noe som gir en direkte og tolkbar representasjon av kontekst. En fordel med denne typen tellevektorer er at de er enkle å forstå og implementere, og at de gir eksplisitt informasjon om hvilke ord som forekommer sammen. En ulempe er at de blir svært store og sparsomme, noe som gir høyt minnebruk og treg beregning. I tillegg fanger de kun opp overflate-samforekomster og ikke dypere semantiske relasjoner mellom ord.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4002c9",
   "metadata": {},
   "source": [
    "### Oppgave 2.4 – Måle distanser med cosinuslikhet (4 poeng)\n",
    "\n",
    "Vi har nå laget en vektorrommodell av ordene i datasettet vårt, der hver kolonne representerer et ord i vokabularet vårt. Vi skal nå forsøke å tallfeste hvor like to vektorer, så vi kan se om dette stemmer overens med likheter mellom ord.\n",
    "Dette skal vi gjøre ved bruk av cosinuslikhet (engelsk: _cosine similarity_), der vi måler vinkelen mellom to vektorer.\n",
    "\n",
    "Cosinuslikheten mellom to vektorer $a$ og $b$ er definert som:\n",
    "\n",
    "$$\n",
    "cos(a,\\ b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "$$\n",
    "\n",
    "Her er $a \\cdot b$ prikkproduktet av de to vektorene, og $\\|a\\|$ og $\\|b\\|$ er normen (lengden) til henholdsvis vektor $a$ og $b$.\n",
    "\n",
    "Dere skal implementere funksjonen for cosinuslikhet på egenhånd. Det vil si at dere ikke skal importere ferdige funksjoner, men heller implementere dette direkte i Python.\n",
    "Dere kan derimot bruke pakken `math`, som har en rekke nyttige funksjoner for numeriske beregninger.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Implementere funksjonen `cosine_similarity(a, b)` som tar inn to vektorer og returnerer cosinuslikheten.\n",
    "- Bruke funksjonen til å finne cosinuslikheten mellom to ord dere selv velger. Skriv en kort kommentar på hva dere observerer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedda83",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 2.4**\n",
    "Bruker cosinus likhet for å måle likheten mellom to ordvektorer. Dette måler vinkelen mellom to vektorer, uten å  bruke lengden. cosinuslikhet formelen er gitt ved: \n",
    "$$\n",
    "cos(a,\\ b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|}\n",
    "$$\n",
    "Prikkproduktet er a*b og normen er sqrt(a^2). \n",
    "Dette blir implementert \"for hånd\" uten å bruke allerede eksisterende funksjoner. \n",
    "Det implementeres ved bruk av løkker og Math.sqrt() (square root). \n",
    "Fikk feilmelding på division by zero, så la inn en division by zero sjekk, som tar seg av dette for nullvektorer: if denominator = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f9cc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppgave 2.4\n",
    "import math\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    # Regn ut cosinuslikhet her\n",
    "    dot = 0.0\n",
    "    norm_a = 0.0\n",
    "    norm_b = 0.0\n",
    "\n",
    "    # Kalkulerer prikk produktet og normene\n",
    "    for x, y in zip(a, b):\n",
    "        x = float(x)\n",
    "        y = float(y)\n",
    "        dot += x * y\n",
    "        norm_a += x * x\n",
    "        norm_b += y * y\n",
    "\n",
    "    denominator = math.sqrt(norm_a) * math.sqrt(norm_b)\n",
    "\n",
    "    # Division by zero check\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Likhet\n",
    "    return dot / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f46cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosinus likheten: 0.907\n"
     ]
    }
   ],
   "source": [
    "# Eksempelbruk:\n",
    "vec1 = vectorizer.get_vector(\"Norge\")\n",
    "vec2 = vectorizer.get_vector(\"Sverige\")\n",
    "\n",
    "likhet_res = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosinus likheten: {likhet_res:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724af4f",
   "metadata": {},
   "source": [
    "### Oppgave 2.5 – Finne likheter mellom ord (5 poeng)\n",
    "\n",
    "Vi skal nå bruke cosinuslikhet til å finne ord som er like hverandre i vektorrommet vårt.\n",
    "I cellen under finner dere to sett med ord i listene `WORDS` og `WORDS_SUBSET`. For hvert av ordene i `WORDS_SUBSET` skal dere finne de fem ordene fra `WORDS` som er likest, målt med cosinuslikhet.  \n",
    "Lag en tabell i en markdown-celle der dere setter inn hvert ord i en kolonne og de fem likeste ordene i en annen kolonne. \n",
    "Dette kan dere gjøre på følgende måte:\n",
    "\n",
    "```md\n",
    "| Ord    | 5 likeste |\n",
    "| ------ | --------- |\n",
    "| hund   | ...       |\n",
    "| salat  | ...       |\n",
    "| bil    | ...       |\n",
    "...\n",
    "```\n",
    "\n",
    "Diskuter kort resultatene dere får. Virker det som at likere ord får høyere cosinuslikhet? Kom med noen eksempler og/eller moteksempler.\n",
    "\n",
    "I denne oppgaven skal dere:\n",
    "\n",
    "- Regne ut cosinuslikhet mellom hvert ord i `WORDS_SUBSET` og alle ordene i `WORDS`.\n",
    "- Lage en tabell med de fem likeste ordene for hvert ord. \n",
    "- Diskutere resultatene deres. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b8ef6",
   "metadata": {},
   "source": [
    "### **Kommentarer og begrunnelser Oppgave 2.5**\n",
    "I denne siste oppgaven, så brukes cosinuslikhet funksjonen til å finne de 5 mest like ordene for hvert element i WORDS_SUBSET listen, mot alle ordene i WORDS listen. Target-ord sin vektor blir sammelignet med kandidat-ord sin vektor. Parene blir lagerte på denne formen: (ord, likhet) i en ny liste, der de er sortert descending. Til slutt printes en emulert Markdown tabell slik som det blir bedt om i oppgaveteksten.\n",
    "Denne oppgaven gir en fin og oversiktlig måte å finne ut om vektorrommet fanger opp sematiske sammenhger, slik som om en katt er lik en hund, osv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdcdbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = [\n",
    "    \"gris\", \"elefant\", \"hund\", \"katt\", \"hest\", \"bjørn\", \"kylling\", \"svin\",\n",
    "    \"fisk\", \"salat\", \"brød\", \"ost\", \"smør\", \"melk\", \"bil\", \"moped\", \"sykkel\",\n",
    "    \"båt\", \"fly\", \"tog\", \"sykepleier\", \"lege\", \"ingeniør\", \"sjef\",\n",
    "    \"sjåfør\", \"designer\", \"jente\", \"gutt\", \"mann\", \"kvinne\", \"Oslo\", \"Trondheim\",\n",
    "    \"Roma\", \"Egypt\", \"Mexico\", \"Galdhøpiggen\", \"Sørlandet\", \"Sahara\", \"Bjørn\",\n",
    "    \"Anne\", \"Jonas\", \"Erna\", \"Donald\", \"Barack\", \"Mohamed\", \"Angela\"\n",
    "]\n",
    "WORDS_SUBSET = [\n",
    "    \"hund\", \"salat\", \"bil\", \"ingeniør\", \"sjef\", \"jente\", \"Oslo\", \"Jonas\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947277d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Ord | 5 likeste (med cosinus likhet) |\n",
      "| --- | --- |\n",
      "| hund | hest (0.978), båt (0.977), katt (0.976), bil (0.973), gris (0.971) |\n",
      "| salat | ost (0.954), brød (0.947), kylling (0.947), smør (0.947), melk (0.901) |\n",
      "| bil | båt (0.982), hund (0.973), sjåfør (0.971), mann (0.965), hest (0.962) |\n",
      "| ingeniør | designer (0.968), lege (0.964), sykepleier (0.957), hest (0.955), gris (0.955) |\n",
      "| sjef | ingeniør (0.939), lege (0.939), fisk (0.936), Anne (0.930), designer (0.929) |\n",
      "| jente | gutt (0.970), kvinne (0.963), mann (0.960), bil (0.916), sjåfør (0.913) |\n",
      "| Oslo | Trondheim (0.990), Mexico (0.968), Egypt (0.967), Roma (0.953), Anne (0.944) |\n",
      "| Jonas | Anne (0.777), Bjørn (0.775), Egypt (0.767), lege (0.767), fisk (0.764) |\n"
     ]
    }
   ],
   "source": [
    "# Oppgave 2.5\n",
    "def top_5_similar(target_word, words, vectorizer):\n",
    "    target_vec = vectorizer.get_vector(target_word)\n",
    "\n",
    "    like = []\n",
    "    for word in words:\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_vec = vectorizer.get_vector(word)\n",
    "        lik = cosine_similarity(target_vec, word_vec)\n",
    "        like.append((word, lik))\n",
    "\n",
    "    # Sort on desc\n",
    "    like.sort(key=lambda x: x[1], reverse=True)\n",
    "    return like[:5]\n",
    "\n",
    "resultater = {}\n",
    "for words in WORDS_SUBSET:\n",
    "    resultater[words] = top_5_similar(words, WORDS, vectorizer)\n",
    "\n",
    "\n",
    "print(\"| Ord | 5 likeste (med cosinus likhet) |\")\n",
    "print(\"| --- | --- |\")\n",
    "\n",
    "for words in WORDS_SUBSET:\n",
    "    top5 = resultater[words]\n",
    "    pretty_pr = \", \".join([f\"{word} ({lik:.3f})\" for word, lik in top5])\n",
    "    print(f\"| {words} | {pretty_pr} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7d9da",
   "metadata": {},
   "source": [
    "#### Oppgave 2.5 – Tekstbesvarelse\n",
    "\n",
    "_Legg inn tabellen din her. Hva ser du av resultatene?_\n",
    "\n",
    "**Svar:**\n",
    "Tabellen jeg får:\n",
    "| Ord | 5 likeste (med cosinus) |\n",
    "| --- | --- |\n",
    "| hund | hest (0.978), båt (0.977), katt (0.976), bil (0.973), gris (0.971) |\n",
    "| salat | ost (0.954), brød (0.947), kylling (0.947), smør (0.947), melk (0.901) |\n",
    "| bil | båt (0.982), hund (0.973), sjåfør (0.971), mann (0.965), hest (0.962) |\n",
    "| ingeniør | designer (0.968), lege (0.964), sykepleier (0.957), hest (0.955), gris (0.955) |\n",
    "| sjef | ingeniør (0.939), lege (0.939), fisk (0.936), Anne (0.930), designer (0.929) |\n",
    "| jente | gutt (0.970), kvinne (0.963), mann (0.960), bil (0.916), sjåfør (0.913) |\n",
    "| Oslo | Trondheim (0.990), Mexico (0.968), Egypt (0.967), Roma (0.953), Anne (0.944) |\n",
    "| Jonas | Anne (0.777), Bjørn (0.775), Egypt (0.767), lege (0.767), fisk (0.764) |\n",
    "\n",
    "Resultatene viser at ord som er semantisk relaterte ofte får høy cosinuslikhet. For eksempel er hund mest lik hest, katt og gri, som alle er dyr, og salat er mest lik ost, brød og smør som alle er matvarer. Tilsvarende ser vi at jente er mest lik gutt, kvinne og mann, som er forventet. Stedsnavn som Oslo er mest lik andre stedsnavn som Trondheim og Roma. Samtidig finnes det enkelte mindre intuitive sammenhenger, som at infeniør er relativt lik hest og gris, noe som jeg går ut ifra er tilfeldige sammenhenger som har forekommet i datasettet.\n",
    "\n",
    "For å konkludere så er resultatene for cosinuslikhet i de tellerbaserte vektorene representasjoner for meningsfulle relasjoner i mange tilfeller, men de kan også inneholde uønsket og uforutsett støy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391aef09",
   "metadata": {},
   "source": [
    "## NB!\n",
    "\n",
    "Husk å kjøre gjennom hele notebooken fra toppen av før dere leverer obligen! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
